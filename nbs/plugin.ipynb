{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64c2732",
   "metadata": {},
   "source": [
    "# Whisper Plugin\n",
    "\n",
    "> Plugin implementation for OpenAI Whisper transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from dataclasses import replace as dataclass_replace\n",
    "from typing import Dict, Any, Optional, List, Union\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    import whisper\n",
    "    from whisper import load_model\n",
    "    from whisper import transcribe\n",
    "    from cjm_ffmpeg_utils.core import FFMPEG_AVAILABLE\n",
    "    WHISPER_AVAILABLE = True and FFMPEG_AVAILABLE\n",
    "except ImportError:\n",
    "    WHISPER_AVAILABLE = False\n",
    "    \n",
    "from cjm_transcription_plugin_system.plugin_interface import TranscriptionPlugin\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult\n",
    "from cjm_plugin_system.utils.validation import (\n",
    "    dict_to_config, config_to_dict, validate_config, dataclass_to_jsonschema,\n",
    "    SCHEMA_TITLE, SCHEMA_DESC, SCHEMA_MIN, SCHEMA_MAX, SCHEMA_ENUM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class WhisperPluginConfig:\n",
    "    \"\"\"Configuration for Whisper transcription plugin.\"\"\"\n",
    "    model:str = field(\n",
    "        default=\"base\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Model\",\n",
    "            SCHEMA_DESC: \"Whisper model size. Larger models are more accurate but slower.\",\n",
    "            SCHEMA_ENUM: [\"tiny\", \"tiny.en\", \"base\", \"base.en\", \"small\", \"small.en\", \n",
    "                        \"medium\", \"medium.en\", \"large\", \"large-v1\", \"large-v2\", \"large-v3\"]\n",
    "        }\n",
    "    )\n",
    "    device:str = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Device\",\n",
    "            SCHEMA_DESC: \"Device for inference (auto will use CUDA if available)\",\n",
    "            SCHEMA_ENUM: [\"auto\", \"cpu\", \"cuda\"]\n",
    "        }\n",
    "    )\n",
    "    language:Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Language\",\n",
    "            SCHEMA_DESC: \"Language code (e.g., 'en', 'es', 'fr') or None for auto-detection\"\n",
    "        }\n",
    "    )\n",
    "    task:str = field(\n",
    "        default=\"transcribe\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Task\",\n",
    "            SCHEMA_DESC: \"Task to perform (transcribe or translate to English)\",\n",
    "            SCHEMA_ENUM: [\"transcribe\", \"translate\"]\n",
    "        }\n",
    "    )\n",
    "    temperature:float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Temperature\",\n",
    "            SCHEMA_DESC: \"Sampling temperature (0 for deterministic)\",\n",
    "            SCHEMA_MIN: 0.0,\n",
    "            SCHEMA_MAX: 1.0\n",
    "        }\n",
    "    )\n",
    "    temperature_increment_on_fallback:Optional[float] = field(\n",
    "        default=0.2,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Temperature Increment\",\n",
    "            SCHEMA_DESC: \"Temperature increment when falling back\",\n",
    "            SCHEMA_MIN: 0.0,\n",
    "            SCHEMA_MAX: 1.0\n",
    "        }\n",
    "    )\n",
    "    beam_size:int = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Beam Size\",\n",
    "            SCHEMA_DESC: \"Beam search width\",\n",
    "            SCHEMA_MIN: 1,\n",
    "            SCHEMA_MAX: 10\n",
    "        }\n",
    "    )\n",
    "    best_of:int = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Best Of\",\n",
    "            SCHEMA_DESC: \"Number of candidates when sampling\",\n",
    "            SCHEMA_MIN: 1,\n",
    "            SCHEMA_MAX: 10\n",
    "        }\n",
    "    )\n",
    "    patience:float = field(\n",
    "        default=1.0,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Patience\",\n",
    "            SCHEMA_DESC: \"Beam search patience factor\",\n",
    "            SCHEMA_MIN: 0.0,\n",
    "            SCHEMA_MAX: 2.0\n",
    "        }\n",
    "    )\n",
    "    length_penalty:Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Length Penalty\",\n",
    "            SCHEMA_DESC: \"Exponential length penalty\"\n",
    "        }\n",
    "    )\n",
    "    suppress_tokens:str = field(\n",
    "        default=\"-1\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Suppress Tokens\",\n",
    "            SCHEMA_DESC: \"Tokens to suppress ('-1' for default)\"\n",
    "        }\n",
    "    )\n",
    "    initial_prompt:Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Initial Prompt\",\n",
    "            SCHEMA_DESC: \"Optional initial prompt\"\n",
    "        }\n",
    "    )\n",
    "    condition_on_previous_text:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Condition on Previous\",\n",
    "            SCHEMA_DESC: \"Condition on previous text\"\n",
    "        }\n",
    "    )\n",
    "    fp16:bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"FP16\",\n",
    "            SCHEMA_DESC: \"Use FP16 (half precision) for faster inference\"\n",
    "        }\n",
    "    )\n",
    "    compression_ratio_threshold:float = field(\n",
    "        default=2.4,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Compression Ratio Threshold\",\n",
    "            SCHEMA_DESC: \"Threshold for repetition detection\",\n",
    "            SCHEMA_MIN: 1.0,\n",
    "            SCHEMA_MAX: 10.0\n",
    "        }\n",
    "    )\n",
    "    logprob_threshold:float = field(\n",
    "        default=-1.0,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Logprob Threshold\",\n",
    "            SCHEMA_DESC: \"Average log probability threshold\"\n",
    "        }\n",
    "    )\n",
    "    no_speech_threshold:float = field(\n",
    "        default=0.6,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"No Speech Threshold\",\n",
    "            SCHEMA_DESC: \"Threshold for silence detection\",\n",
    "            SCHEMA_MIN: 0.0,\n",
    "            SCHEMA_MAX: 1.0\n",
    "        }\n",
    "    )\n",
    "    word_timestamps:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Word Timestamps\",\n",
    "            SCHEMA_DESC: \"Extract word-level timestamps\"\n",
    "        }\n",
    "    )\n",
    "    prepend_punctuations:str = field(\n",
    "        default=\"\\\"'“¿([{-\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Prepend Punctuations\",\n",
    "            SCHEMA_DESC: \"Punctuations to merge with next word\"\n",
    "        }\n",
    "    )\n",
    "    append_punctuations:str = field(\n",
    "        default=\"\\\"'.。,，!！?？:：”)]}、\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Append Punctuations\",\n",
    "            SCHEMA_DESC: \"Punctuations to merge with previous word\"\n",
    "        }\n",
    "    )\n",
    "    threads:int = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Threads\",\n",
    "            SCHEMA_DESC: \"Number of threads (0 for default)\",\n",
    "            SCHEMA_MIN: 0\n",
    "        }\n",
    "    )\n",
    "    model_dir:Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Model Directory\",\n",
    "            SCHEMA_DESC: \"Directory to save/load models\"\n",
    "        }\n",
    "    )\n",
    "    compile_model:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Compile Model\",\n",
    "            SCHEMA_DESC: \"Use torch.compile for potential speedup (requires PyTorch 2.0+)\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "class WhisperLocalPlugin(TranscriptionPlugin):\n",
    "    \"\"\"OpenAI Whisper transcription plugin.\"\"\"\n",
    "    \n",
    "    config_class = WhisperPluginConfig\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Whisper plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config: WhisperPluginConfig = None\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.model_dir = None\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str: # Plugin name identifier\n",
    "        \"\"\"Get the plugin name identifier.\"\"\"\n",
    "        return \"whisper_local\"\n",
    "    \n",
    "    @property\n",
    "    def version(self) -> str: # Plugin version string\n",
    "        \"\"\"Get the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(self) -> List[str]: # List of supported audio file formats\n",
    "        \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "        return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "\n",
    "    def get_current_config(self) -> Dict[str, Any]: # Current configuration as dictionary\n",
    "        \"\"\"Return current configuration state.\"\"\"\n",
    "        if not self.config:\n",
    "            return {}\n",
    "        return config_to_dict(self.config)\n",
    "\n",
    "    def get_config_schema(self) -> Dict[str, Any]: # JSON Schema for configuration\n",
    "        \"\"\"Return JSON Schema for UI generation.\"\"\"\n",
    "        return dataclass_to_jsonschema(WhisperPluginConfig)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_dataclass() -> WhisperPluginConfig: # Configuration dataclass\n",
    "        \"\"\"Return dataclass describing the plugin's configuration options.\"\"\"\n",
    "        return WhisperPluginConfig\n",
    "    \n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Any] = None # Configuration dataclass, dict, or None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize or re-configure the plugin (idempotent).\"\"\"\n",
    "        # Parse new config\n",
    "        new_config = dict_to_config(WhisperPluginConfig, config or {})\n",
    "        \n",
    "        # Check for changes if already running\n",
    "        if self.config:\n",
    "            # If the model selection changed, unload old model\n",
    "            if self.config.model != new_config.model:\n",
    "                self.logger.info(f\"Config change: Model {self.config.model} -> {new_config.model}\")\n",
    "                self._unload_model()\n",
    "            \n",
    "            # If device changed, unload\n",
    "            if self.config.device != new_config.device:\n",
    "                self.logger.info(f\"Config change: Device {self.config.device} -> {new_config.device}\")\n",
    "                self._unload_model()\n",
    "        \n",
    "        # Apply new config\n",
    "        self.config = new_config\n",
    "        \n",
    "        # Set device\n",
    "        if self.config.device == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = self.config.device\n",
    "        \n",
    "        # Set model directory\n",
    "        self.model_dir = self.config.model_dir\n",
    "        \n",
    "        self.logger.info(f\"Initialized Whisper plugin with model '{self.config.model}' on device '{self.device}'\")\n",
    "    \n",
    "    def _unload_model(self) -> None:\n",
    "        \"\"\"Unload the current model and free resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.logger.info(\"Unloading Whisper model for reconfiguration\")\n",
    "            self.model = None\n",
    "            \n",
    "            # Clear GPU cache if using CUDA\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    def _load_model(self) -> None:\n",
    "        \"\"\"Load the Whisper model (lazy loading).\"\"\"\n",
    "        if self.model is None:\n",
    "            try:\n",
    "                self.logger.info(f\"Loading Whisper model: {self.config.model}\")\n",
    "                self.model = load_model(\n",
    "                    self.config.model, \n",
    "                    device=self.device,\n",
    "                    download_root=self.model_dir\n",
    "                )\n",
    "                \n",
    "                # Optionally compile the model (PyTorch 2.0+)\n",
    "                if self.config.compile_model and hasattr(torch, 'compile'):\n",
    "                    self.model = torch.compile(self.model)\n",
    "                    self.logger.info(\"Model compiled with torch.compile\")\n",
    "                    \n",
    "                self.logger.info(\"Local Whisper model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load Whisper model: {e}\")\n",
    "    \n",
    "    def _prepare_audio(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path] # Audio data, file path, or Path object to prepare\n",
    "    ) -> str: # Path to the prepared audio file\n",
    "        \"\"\"Prepare audio for Whisper processing.\"\"\"\n",
    "        if isinstance(audio, (str, Path)):\n",
    "            # Already a file path\n",
    "            return str(audio)\n",
    "        \n",
    "        elif isinstance(audio, AudioData):\n",
    "            # Save AudioData to temporary file\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "                # Ensure audio is in the correct format\n",
    "                audio_array = audio.samples\n",
    "                \n",
    "                # If stereo, convert to mono\n",
    "                if audio_array.ndim > 1:\n",
    "                    audio_array = audio_array.mean(axis=1)\n",
    "                \n",
    "                # Ensure float32 and normalized\n",
    "                if audio_array.dtype != np.float32:\n",
    "                    audio_array = audio_array.astype(np.float32)\n",
    "                \n",
    "                # Normalize if needed\n",
    "                if audio_array.max() > 1.0:\n",
    "                    audio_array = audio_array / np.abs(audio_array).max()\n",
    "                \n",
    "                # Save to file\n",
    "                sf.write(tmp_file.name, audio_array, audio.sample_rate)\n",
    "                return tmp_file.name\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n",
    "    \n",
    "    def _save_to_db(\n",
    "        self,\n",
    "        result: TranscriptionResult # Transcription result to save\n",
    "    ) -> None:\n",
    "        \"\"\"Save transcription result to database (placeholder).\"\"\"\n",
    "        # Placeholder for DB logic\n",
    "        # Implementation will use self.db_path which can be injected via config or environment\n",
    "        pass\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path], # Audio data or path to audio file to transcribe\n",
    "        **kwargs # Additional arguments to override config\n",
    "    ) -> TranscriptionResult: # Transcription result with text and metadata\n",
    "        \"\"\"Transcribe audio using Whisper.\"\"\"\n",
    "        # Load model if not already loaded\n",
    "        self._load_model()\n",
    "        \n",
    "        # Prepare audio file (handles Zero-Copy handoff from Proxy)\n",
    "        audio_path = self._prepare_audio(audio)\n",
    "        temp_file_created = (audio_path != str(audio)) and not isinstance(audio, (str, Path))\n",
    "        \n",
    "        try:\n",
    "            # Get config values, allowing kwargs overrides\n",
    "            model_name = kwargs.get(\"model\", self.config.model)\n",
    "            task = kwargs.get(\"task\", self.config.task)\n",
    "            language = kwargs.get(\"language\", self.config.language)\n",
    "            beam_size = kwargs.get(\"beam_size\", self.config.beam_size)\n",
    "            best_of = kwargs.get(\"best_of\", self.config.best_of)\n",
    "            patience = kwargs.get(\"patience\", self.config.patience)\n",
    "            length_penalty = kwargs.get(\"length_penalty\", self.config.length_penalty)\n",
    "            suppress_tokens = kwargs.get(\"suppress_tokens\", self.config.suppress_tokens)\n",
    "            initial_prompt = kwargs.get(\"initial_prompt\", self.config.initial_prompt)\n",
    "            condition_on_previous_text = kwargs.get(\"condition_on_previous_text\", self.config.condition_on_previous_text)\n",
    "            fp16 = kwargs.get(\"fp16\", self.config.fp16)\n",
    "            compression_ratio_threshold = kwargs.get(\"compression_ratio_threshold\", self.config.compression_ratio_threshold)\n",
    "            logprob_threshold = kwargs.get(\"logprob_threshold\", self.config.logprob_threshold)\n",
    "            no_speech_threshold = kwargs.get(\"no_speech_threshold\", self.config.no_speech_threshold)\n",
    "            word_timestamps = kwargs.get(\"word_timestamps\", self.config.word_timestamps)\n",
    "            prepend_punctuations = kwargs.get(\"prepend_punctuations\", self.config.prepend_punctuations)\n",
    "            append_punctuations = kwargs.get(\"append_punctuations\", self.config.append_punctuations)\n",
    "            temperature = kwargs.get(\"temperature\", self.config.temperature)\n",
    "            temp_increment = kwargs.get(\"temperature_increment_on_fallback\", self.config.temperature_increment_on_fallback)\n",
    "            threads = kwargs.get(\"threads\", self.config.threads)\n",
    "            \n",
    "            # Prepare Whisper arguments\n",
    "            whisper_args = {\n",
    "                \"verbose\": False,\n",
    "                \"task\": task,\n",
    "                \"language\": language,\n",
    "                \"beam_size\": beam_size,\n",
    "                \"best_of\": best_of,\n",
    "                \"patience\": patience,\n",
    "                \"length_penalty\": length_penalty,\n",
    "                \"suppress_tokens\": suppress_tokens,\n",
    "                \"initial_prompt\": initial_prompt,\n",
    "                \"condition_on_previous_text\": condition_on_previous_text,\n",
    "                \"fp16\": fp16 and self.device == \"cuda\",\n",
    "                \"compression_ratio_threshold\": compression_ratio_threshold,\n",
    "                \"logprob_threshold\": logprob_threshold,\n",
    "                \"no_speech_threshold\": no_speech_threshold,\n",
    "                \"word_timestamps\": word_timestamps,\n",
    "                \"prepend_punctuations\": prepend_punctuations,\n",
    "                \"append_punctuations\": append_punctuations,\n",
    "            }\n",
    "            \n",
    "            # Handle temperature settings\n",
    "            if temp_increment is not None and temp_increment > 0:\n",
    "                temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temp_increment))\n",
    "            else:\n",
    "                temperature = [temperature]\n",
    "            \n",
    "            # Set number of threads if specified\n",
    "            if threads > 0:\n",
    "                torch.set_num_threads(threads)\n",
    "            \n",
    "            # Perform transcription\n",
    "            self.logger.info(f\"Transcribing audio with Whisper {model_name}\")\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")  # Suppress Whisper warnings\n",
    "                result = transcribe(\n",
    "                    self.model,\n",
    "                    audio_path,\n",
    "                    temperature=temperature,\n",
    "                    **whisper_args\n",
    "                )\n",
    "            \n",
    "            # Process segments\n",
    "            segments = []\n",
    "            for segment in result.get(\"segments\", []):\n",
    "                segment_data = {\n",
    "                    \"start\": segment[\"start\"],\n",
    "                    \"end\": segment[\"end\"],\n",
    "                    \"text\": segment[\"text\"].strip()\n",
    "                }\n",
    "                \n",
    "                # Add word timestamps if available\n",
    "                if \"words\" in segment and word_timestamps:\n",
    "                    segment_data[\"words\"] = [\n",
    "                        {\n",
    "                            \"word\": word[\"word\"],\n",
    "                            \"start\": word[\"start\"],\n",
    "                            \"end\": word[\"end\"],\n",
    "                            \"probability\": word.get(\"probability\")\n",
    "                        }\n",
    "                        for word in segment[\"words\"]\n",
    "                    ]\n",
    "                \n",
    "                segments.append(segment_data)\n",
    "            \n",
    "            # Create transcription result\n",
    "            transcription_result = TranscriptionResult(\n",
    "                text=result[\"text\"].strip(),\n",
    "                confidence=None,  # Whisper doesn't provide overall confidence\n",
    "                segments=segments if segments else None,\n",
    "                metadata={\n",
    "                    \"model\": model_name,\n",
    "                    \"language\": result.get(\"language\", language),\n",
    "                    \"task\": task,\n",
    "                    \"device\": self.device,\n",
    "                    \"duration\": result.get(\"duration\"),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Save to database (placeholder)\n",
    "            self._save_to_db(transcription_result)\n",
    "            \n",
    "            self.logger.info(f\"Transcription completed: {len(result['text'].split())} words\")\n",
    "            return transcription_result\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file if created\n",
    "            if temp_file_created:\n",
    "                try:\n",
    "                    Path(audio_path).unlink(missing_ok=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    def is_available(self) -> bool: # True if Whisper and its dependencies are available\n",
    "        \"\"\"Check if Whisper is available.\"\"\"\n",
    "        return WHISPER_AVAILABLE\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.logger.info(\"Unloading Whisper model\")\n",
    "            self.model = None\n",
    "            \n",
    "            # Clear GPU cache if using CUDA\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            self.logger.info(\"Cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21850c5c",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper available: True\n",
      "Plugin name: whisper_local\n",
      "Plugin version: 1.0.0\n",
      "Supported formats: ['wav', 'mp3', 'flac', 'm4a', 'ogg', 'webm', 'mp4', 'avi', 'mov']\n",
      "Config class: WhisperPluginConfig\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = WhisperLocalPlugin()\n",
    "\n",
    "# Check availability\n",
    "print(f\"Whisper available: {plugin.is_available()}\")\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Supported formats: {plugin.supported_formats}\")\n",
    "print(f\"Config class: {plugin.config_class.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a45466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - tiny\n",
      "  - tiny.en\n",
      "  - base\n",
      "  - base.en\n",
      "  - small\n",
      "  - small.en\n",
      "  - medium\n",
      "  - medium.en\n",
      "  - large\n",
      "  - large-v1\n",
      "  - large-v2\n",
      "  - large-v3\n"
     ]
    }
   ],
   "source": [
    "# Test configuration dataclass\n",
    "from dataclasses import fields\n",
    "\n",
    "print(\"Available models:\")\n",
    "model_field = next(f for f in fields(WhisperPluginConfig) if f.name == \"model\")\n",
    "for model in model_field.metadata.get(SCHEMA_ENUM, []):\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid config: Valid=True\n",
      "Invalid model: Valid=False\n",
      "  Error: model: 'invalid' is not one of ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium',\n",
      "Temperature out of range: Valid=False\n",
      "  Error: temperature: 1.5 is greater than maximum 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test configuration validation\n",
    "test_configs = [\n",
    "    ({\"model\": \"tiny\"}, \"Valid config\"),\n",
    "    ({\"model\": \"invalid\"}, \"Invalid model\"),\n",
    "    ({\"model\": \"tiny\", \"temperature\": 1.5}, \"Temperature out of range\"),\n",
    "]\n",
    "\n",
    "for config, description in test_configs:\n",
    "    try:\n",
    "        test_cfg = dict_to_config(WhisperPluginConfig, config, validate=True)\n",
    "        print(f\"{description}: Valid=True\")\n",
    "    except ValueError as e:\n",
    "        print(f\"{description}: Valid=False\")\n",
    "        print(f\"  Error: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36673bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config (dict): {'model': 'tiny', 'device': 'cpu', 'language': None, 'task': 'transcribe', 'temperature': 0.0, 'temperature_increment_on_fallback': 0.2, 'beam_size': 5, 'best_of': 5, 'patience': 1.0, 'length_penalty': None, 'suppress_tokens': '-1', 'initial_prompt': None, 'condition_on_previous_text': False, 'fp16': True, 'compression_ratio_threshold': 2.4, 'logprob_threshold': -1.0, 'no_speech_threshold': 0.6, 'word_timestamps': False, 'prepend_punctuations': '\"\\'“¿([{-', 'append_punctuations': '\"\\'.。,，!！?？:：”)]}、', 'threads': 0, 'model_dir': None, 'compile_model': False}\n",
      "Current model: tiny\n"
     ]
    }
   ],
   "source": [
    "# Test initialization and get_current_config (returns dict now)\n",
    "plugin.initialize({\"model\": \"tiny\", \"device\": \"cpu\"})\n",
    "current_config = plugin.get_current_config()\n",
    "print(f\"Current config (dict): {current_config}\")\n",
    "print(f\"Current model: {current_config['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remk25y2ny9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Schema for WhisperPluginConfig:\n",
      "  Name: WhisperPluginConfig\n",
      "  Properties count: 23\n",
      "  Model field enum: ['tiny', 'tiny.en', 'base']...\n",
      "\n",
      "Full schema (truncated):\n",
      "{\n",
      "  \"model\": {\n",
      "    \"type\": \"string\",\n",
      "    \"title\": \"Model\",\n",
      "    \"description\": \"Whisper model size. Larger models are more accurate but slower.\",\n",
      "    \"enum\": [\n",
      "      \"tiny\",\n",
      "      \"tiny.en\",\n",
      "      \"base\",\n",
      "      \"base.en\",\n",
      "      \"small\",\n",
      "      \"small.en\",\n",
      "      \"medium\",\n",
      "      \"medium.en\",\n",
      "      \"large\",\n",
      "      \"large-v1\",\n",
      "      \"large-v2\",\n",
      "      \"large-v3\"\n",
      "    ],\n",
      "    \"default\": \"base\"\n",
      "  },\n",
      "  \"device\": {\n",
      "    \"type\": \"string\",\n",
      "    \"title\": \"Device\",\n",
      "    \"description\": \"Device for inference (auto will use CUDA if available)\",\n",
      "    \"enum\": [\n",
      "      \"auto\",\n",
      "      \"cpu\",\n",
      "      \"cuda\"\n",
      "    ],\n",
      "    \"default\": \"auto\"\n",
      "  },\n",
      "  \"language\": {\n",
      "    \"type\": [\n",
      "      \"string\",\n",
      "      \"null\"\n",
      "    ],\n",
      "    \"title\": \"Language\",\n",
      "    \"description\": \"Language code (e.g., 'en', 'es', 'fr') or None for auto-detection\",\n",
      "    \"default\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test get_config_schema for UI generation\n",
    "import json\n",
    "\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"JSON Schema for WhisperPluginConfig:\")\n",
    "print(f\"  Name: {schema['name']}\")\n",
    "print(f\"  Properties count: {len(schema['properties'])}\")\n",
    "print(f\"  Model field enum: {schema['properties']['model'].get('enum', [])[:3]}...\")\n",
    "print(f\"\\nFull schema (truncated):\")\n",
    "print(json.dumps({k: v for k, v in list(schema['properties'].items())[:3]}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9sxy0nyzpod",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__.WhisperLocalPlugin:Initialized Whisper plugin with model 'tiny' on device 'cpu'\n",
      "INFO:__main__.WhisperLocalPlugin:Initialized Whisper plugin with model 'tiny' on device 'cpu'\n",
      "INFO:__main__.WhisperLocalPlugin:Config change: Model tiny -> base\n",
      "INFO:__main__.WhisperLocalPlugin:Initialized Whisper plugin with model 'base' on device 'cpu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial config: model=tiny\n",
      "\n",
      "Re-initializing with same model...\n",
      "\n",
      "Re-initializing with different model...\n",
      "New config: model=base\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test idempotent initialize - model unload on config change\n",
    "import logging\n",
    "\n",
    "# Enable logging to see model unload messages\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize with one model\n",
    "plugin.initialize({\"model\": \"tiny\", \"device\": \"cpu\"})\n",
    "print(f\"Initial config: model={plugin.config.model}\")\n",
    "\n",
    "# Re-initialize with same model (no unload should happen)\n",
    "print(\"\\nRe-initializing with same model...\")\n",
    "plugin.initialize({\"model\": \"tiny\", \"device\": \"cpu\"})\n",
    "\n",
    "# Re-initialize with different model (unload should trigger)\n",
    "print(\"\\nRe-initializing with different model...\")\n",
    "plugin.initialize({\"model\": \"base\", \"device\": \"cpu\"})\n",
    "print(f\"New config: model={plugin.config.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f8542-fa22-421d-abba-b589e71383d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
