{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64c2732",
   "metadata": {},
   "source": [
    "# Whisper Plugin\n",
    "\n",
    "> Plugin implementation for OpenAI Whisper transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Union\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    import whisper\n",
    "    from whisper import load_model\n",
    "    from whisper import transcribe\n",
    "    from cjm_ffmpeg_utils.core import FFMPEG_AVAILABLE\n",
    "    WHISPER_AVAILABLE = True and FFMPEG_AVAILABLE\n",
    "except ImportError:\n",
    "    WHISPER_AVAILABLE = False\n",
    "    \n",
    "from cjm_transcription_plugin_system.plugin_interface import TranscriptionPlugin\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WhisperLocalPlugin(TranscriptionPlugin):\n",
    "    \"\"\"OpenAI Whisper transcription plugin.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Whisper plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config = {}\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.model_dir = None\n",
    "    \n",
    "    @property\n",
    "    def name(\n",
    "        self\n",
    "    ) -> str: # the plugin name identifier\n",
    "        \"\"\"Get the plugin name identifier.\"\"\"\n",
    "        return \"whisper_local\"\n",
    "    \n",
    "    @property\n",
    "    def version(\n",
    "        self\n",
    "    ) -> str: # the plugin version string\n",
    "        \"\"\"Get the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(\n",
    "        self\n",
    "    ) -> List[str]: # list of supported audio file formats\n",
    "        \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "        return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_schema(\n",
    "    ) -> Dict[str, Any]: # the configuration schema dictionary\n",
    "        \"\"\"Return configuration schema for Whisper.\"\"\"\n",
    "        return {\n",
    "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"Whisper Configuration\",\n",
    "            \"properties\": {\n",
    "                \"model\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"tiny\", \"tiny.en\", \"base\", \"base.en\", \"small\", \"small.en\", \n",
    "                            \"medium\", \"medium.en\", \"large\", \"large-v1\", \"large-v2\", \"large-v3\"],\n",
    "                    \"default\": \"base\",\n",
    "                    \"description\": \"Whisper model size. Larger models are more accurate but slower.\"\n",
    "                },\n",
    "                \"device\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"auto\", \"cpu\", \"cuda\"],\n",
    "                    \"default\": \"auto\",\n",
    "                    \"description\": \"Device for inference (auto will use CUDA if available)\"\n",
    "                },\n",
    "                \"language\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Language code (e.g., 'en', 'es', 'fr') or null for auto-detection\"\n",
    "                },\n",
    "                \"task\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"transcribe\", \"translate\"],\n",
    "                    \"default\": \"transcribe\",\n",
    "                    \"description\": \"Task to perform (transcribe or translate to English)\"\n",
    "                },\n",
    "                \"temperature\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.0,\n",
    "                    \"description\": \"Sampling temperature (0 for deterministic)\"\n",
    "                },\n",
    "                \"temperature_increment_on_fallback\": {\n",
    "                    \"type\": [\"number\", \"null\"],\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.2,\n",
    "                    \"description\": \"Temperature increment when falling back\"\n",
    "                },\n",
    "                \"beam_size\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 10,\n",
    "                    \"default\": 5,\n",
    "                    \"description\": \"Beam search width\"\n",
    "                },\n",
    "                \"best_of\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 10,\n",
    "                    \"default\": 5,\n",
    "                    \"description\": \"Number of candidates when sampling\"\n",
    "                },\n",
    "                \"patience\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 2.0,\n",
    "                    \"default\": 1.0,\n",
    "                    \"description\": \"Beam search patience factor\"\n",
    "                },\n",
    "                \"length_penalty\": {\n",
    "                    \"type\": [\"number\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Exponential length penalty\"\n",
    "                },\n",
    "                \"suppress_tokens\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"-1\",\n",
    "                    \"description\": \"Tokens to suppress ('-1' for default)\"\n",
    "                },\n",
    "                \"initial_prompt\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Optional initial prompt\"\n",
    "                },\n",
    "                \"condition_on_previous_text\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Condition on previous text\"\n",
    "                },\n",
    "                \"fp16\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": True,\n",
    "                    \"description\": \"Use FP16 (half precision) for faster inference\"\n",
    "                },\n",
    "                \"compression_ratio_threshold\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 1.0,\n",
    "                    \"maximum\": 10.0,\n",
    "                    \"default\": 2.4,\n",
    "                    \"description\": \"Threshold for repetition detection\"\n",
    "                },\n",
    "                \"logprob_threshold\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"default\": -1.0,\n",
    "                    \"description\": \"Average log probability threshold\"\n",
    "                },\n",
    "                \"no_speech_threshold\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.6,\n",
    "                    \"description\": \"Threshold for silence detection\"\n",
    "                },\n",
    "                \"word_timestamps\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Extract word-level timestamps\"\n",
    "                },\n",
    "                \"prepend_punctuations\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"\\\"'“¿([{-\",\n",
    "                    \"description\": \"Punctuations to merge with next word\"\n",
    "                },\n",
    "                \"append_punctuations\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"\\\"'.。,，!！?？:：”)]}、\",\n",
    "                    \"description\": \"Punctuations to merge with previous word\"\n",
    "                },\n",
    "                \"threads\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 0,\n",
    "                    \"default\": 0,\n",
    "                    \"description\": \"Number of threads (0 for default)\"\n",
    "                },\n",
    "                \"model_dir\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Directory to save/load models\"\n",
    "                },\n",
    "                \"compile_model\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Use torch.compile for potential speedup (requires PyTorch 2.0+)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"model\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    \n",
    "    def get_current_config(\n",
    "        self\n",
    "    ) -> Dict[str, Any]: # the current configuration dictionary\n",
    "        \"\"\"Return current configuration.\"\"\"\n",
    "        defaults = self.get_config_defaults()\n",
    "        return {**defaults, **self.config}\n",
    "    \n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Dict[str, Any]] = None # configuration dictionary to initialize the plugin\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the plugin with configuration.\"\"\"\n",
    "        if config:\n",
    "            is_valid, error = self.validate_config(config)\n",
    "            if not is_valid:\n",
    "                raise ValueError(f\"Invalid configuration: {error}\")\n",
    "        \n",
    "        # Merge with defaults\n",
    "        defaults = self.get_config_defaults()\n",
    "        self.config = {**defaults, **(config or {})}\n",
    "        \n",
    "        # Set device\n",
    "        if self.config[\"device\"] == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = self.config[\"device\"]\n",
    "        \n",
    "        # Set model directory\n",
    "        self.model_dir = self.config.get(\"model_dir\")\n",
    "        \n",
    "        self.logger.info(f\"Initialized Whisper plugin with model '{self.config['model']}' on device '{self.device}'\")\n",
    "    \n",
    "    def _load_model(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"Load the Whisper model (lazy loading).\"\"\"\n",
    "        if self.model is None:\n",
    "            try:\n",
    "                self.logger.info(f\"Loading Whisper model: {self.config['model']}\")\n",
    "                self.model = load_model(\n",
    "                    self.config[\"model\"], \n",
    "                    device=self.device,\n",
    "                    download_root=self.model_dir\n",
    "                )\n",
    "                \n",
    "                # Optionally compile the model (PyTorch 2.0+)\n",
    "                if self.config.get(\"compile_model\", False) and hasattr(torch, 'compile'):\n",
    "                    self.model = torch.compile(self.model)\n",
    "                    self.logger.info(\"Model compiled with torch.compile\")\n",
    "                    \n",
    "                self.logger.info(\"Local Whisper model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load Whisper model: {e}\")\n",
    "    \n",
    "    def _prepare_audio(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path] # audio data, file path, or Path object to prepare\n",
    "    ) -> str: # path to the prepared audio file\n",
    "        \"\"\"Prepare audio for Whisper processing.\"\"\"\n",
    "        if isinstance(audio, (str, Path)):\n",
    "            # Already a file path\n",
    "            return str(audio)\n",
    "        \n",
    "        elif isinstance(audio, AudioData):\n",
    "            # Save AudioData to temporary file\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "                # Ensure audio is in the correct format\n",
    "                audio_array = audio.samples\n",
    "                \n",
    "                # If stereo, convert to mono\n",
    "                if audio_array.ndim > 1:\n",
    "                    audio_array = audio_array.mean(axis=1)\n",
    "                \n",
    "                # Ensure float32 and normalized\n",
    "                if audio_array.dtype != np.float32:\n",
    "                    audio_array = audio_array.astype(np.float32)\n",
    "                \n",
    "                # Normalize if needed\n",
    "                if audio_array.max() > 1.0:\n",
    "                    audio_array = audio_array / np.abs(audio_array).max()\n",
    "                \n",
    "                # Save to file\n",
    "                sf.write(tmp_file.name, audio_array, audio.sample_rate)\n",
    "                return tmp_file.name\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path], # audio data or path to audio file to transcribe\n",
    "        **kwargs # additional arguments to override config\n",
    "    ) -> TranscriptionResult: # transcription result with text and metadata\n",
    "        \"\"\"Transcribe audio using Whisper.\"\"\"\n",
    "        # Load model if not already loaded\n",
    "        self._load_model()\n",
    "        \n",
    "        # Prepare audio file\n",
    "        audio_path = self._prepare_audio(audio)\n",
    "        temp_file_created = not isinstance(audio, (str, Path))\n",
    "        \n",
    "        try:\n",
    "            # Merge runtime kwargs with config\n",
    "            exec_config = {**self.config, **kwargs}\n",
    "            \n",
    "            # Prepare Whisper arguments\n",
    "            whisper_args = {\n",
    "                \"verbose\": False,\n",
    "                \"task\": exec_config[\"task\"],\n",
    "                \"language\": exec_config[\"language\"],\n",
    "                \"beam_size\": exec_config[\"beam_size\"],\n",
    "                \"best_of\": exec_config[\"best_of\"],\n",
    "                \"patience\": exec_config[\"patience\"],\n",
    "                \"length_penalty\": exec_config[\"length_penalty\"],\n",
    "                \"suppress_tokens\": exec_config[\"suppress_tokens\"],\n",
    "                \"initial_prompt\": exec_config[\"initial_prompt\"],\n",
    "                \"condition_on_previous_text\": exec_config[\"condition_on_previous_text\"],\n",
    "                \"fp16\": exec_config[\"fp16\"] and self.device == \"cuda\",\n",
    "                \"compression_ratio_threshold\": exec_config[\"compression_ratio_threshold\"],\n",
    "                \"logprob_threshold\": exec_config[\"logprob_threshold\"],\n",
    "                \"no_speech_threshold\": exec_config[\"no_speech_threshold\"],\n",
    "                \"word_timestamps\": exec_config[\"word_timestamps\"],\n",
    "                \"prepend_punctuations\": exec_config[\"prepend_punctuations\"],\n",
    "                \"append_punctuations\": exec_config[\"append_punctuations\"],\n",
    "            }\n",
    "            \n",
    "            # Handle temperature settings\n",
    "            temperature = exec_config[\"temperature\"]\n",
    "            temp_increment = exec_config[\"temperature_increment_on_fallback\"]\n",
    "            if temp_increment is not None and temp_increment > 0:\n",
    "                temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temp_increment))\n",
    "            else:\n",
    "                temperature = [temperature]\n",
    "            \n",
    "            # Set number of threads if specified\n",
    "            if exec_config[\"threads\"] > 0:\n",
    "                torch.set_num_threads(exec_config[\"threads\"])\n",
    "            \n",
    "            # Perform transcription\n",
    "            self.logger.info(f\"Transcribing audio with Whisper {exec_config['model']}\")\n",
    "            \n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")  # Suppress Whisper warnings\n",
    "                result = transcribe(\n",
    "                    self.model,\n",
    "                    audio_path,\n",
    "                    temperature=temperature,\n",
    "                    **whisper_args\n",
    "                )\n",
    "            \n",
    "            # Process segments\n",
    "            segments = []\n",
    "            for segment in result.get(\"segments\", []):\n",
    "                segment_data = {\n",
    "                    \"start\": segment[\"start\"],\n",
    "                    \"end\": segment[\"end\"],\n",
    "                    \"text\": segment[\"text\"].strip()\n",
    "                }\n",
    "                \n",
    "                # Add word timestamps if available\n",
    "                if \"words\" in segment and exec_config[\"word_timestamps\"]:\n",
    "                    segment_data[\"words\"] = [\n",
    "                        {\n",
    "                            \"word\": word[\"word\"],\n",
    "                            \"start\": word[\"start\"],\n",
    "                            \"end\": word[\"end\"],\n",
    "                            \"probability\": word.get(\"probability\")\n",
    "                        }\n",
    "                        for word in segment[\"words\"]\n",
    "                    ]\n",
    "                \n",
    "                segments.append(segment_data)\n",
    "            \n",
    "            # Create transcription result\n",
    "            transcription_result = TranscriptionResult(\n",
    "                text=result[\"text\"].strip(),\n",
    "                confidence=None,  # Whisper doesn't provide overall confidence\n",
    "                segments=segments if segments else None,\n",
    "                metadata={\n",
    "                    \"model\": exec_config[\"model\"],\n",
    "                    \"language\": result.get(\"language\", exec_config[\"language\"]),\n",
    "                    \"task\": exec_config[\"task\"],\n",
    "                    \"device\": self.device,\n",
    "                    \"duration\": result.get(\"duration\"),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Transcription completed: {len(result['text'].split())} words\")\n",
    "            return transcription_result\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file if created\n",
    "            if temp_file_created:\n",
    "                try:\n",
    "                    Path(audio_path).unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    def is_available(\n",
    "        self\n",
    "    ) -> bool: # True if Whisper and its dependencies are available\n",
    "        \"\"\"Check if Whisper is available.\"\"\"\n",
    "        return WHISPER_AVAILABLE\n",
    "    \n",
    "    def cleanup(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.logger.info(\"Unloading Whisper model\")\n",
    "            self.model = None\n",
    "            \n",
    "            # Clear GPU cache if using CUDA\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            self.logger.info(\"Cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21850c5c",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper available: True\n",
      "Plugin name: whisper_local\n",
      "Plugin version: 1.0.0\n",
      "Supported formats: ['wav', 'mp3', 'flac', 'm4a', 'ogg', 'webm', 'mp4', 'avi', 'mov']\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = WhisperLocalPlugin()\n",
    "\n",
    "# Check availability\n",
    "print(f\"Whisper available: {plugin.is_available()}\")\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Supported formats: {plugin.supported_formats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a45466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - tiny\n",
      "  - tiny.en\n",
      "  - base\n",
      "  - base.en\n",
      "  - small\n",
      "  - small.en\n",
      "  - medium\n",
      "  - medium.en\n",
      "  - large\n",
      "  - large-v1\n",
      "  - large-v2\n",
      "  - large-v3\n"
     ]
    }
   ],
   "source": [
    "# Test configuration schema\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"Available models:\")\n",
    "for model in schema[\"properties\"][\"model\"][\"enum\"]:\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid config: Valid=True\n",
      "Invalid model: Valid=False\n",
      "  Error: 'invalid' is not one of ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium', 'mediu\n",
      "Temperature out of range: Valid=False\n",
      "  Error: 'model' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'$schema': 'http://json\n"
     ]
    }
   ],
   "source": [
    "# Test configuration validation\n",
    "test_configs = [\n",
    "    ({\"model\": \"tiny\"}, \"Valid config\"),\n",
    "    ({\"model\": \"invalid\"}, \"Invalid model\"),\n",
    "    ({\"temperature\": 1.5}, \"Temperature out of range\"),\n",
    "]\n",
    "\n",
    "for config, description in test_configs:\n",
    "    is_valid, error = plugin.validate_config(config)\n",
    "    print(f\"{description}: Valid={is_valid}\")\n",
    "    if error:\n",
    "        print(f\"  Error: {error[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36673bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config: tiny\n"
     ]
    }
   ],
   "source": [
    "# Test initialization\n",
    "plugin.initialize({\"model\": \"tiny\", \"device\": \"cpu\"})\n",
    "print(f\"Current config: {plugin.get_current_config()['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f8542-fa22-421d-abba-b589e71383d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
